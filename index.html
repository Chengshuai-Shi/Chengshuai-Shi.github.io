<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Chengshuai Shi</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Info</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="bio.html">Biography</a></div>
<div class="menu-item"><a href="pub.html">Publications</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Chengshuai Shi</h1>
</div>
<table class="imgtable"><tr><td>
<img src="figs/bb.png" alt="picture" width="200px" height="240px" />&nbsp;</td>
<td align="left"><p>Postdoctoral Fellow
</p>
<p><a href="https://pli.princeton.edu/" target=&ldquo;blank&rdquo;>Princeton Language and Intellegence</a>, <a href="https://www.princeton.edu/" target=&ldquo;blank&rdquo;>Princeton University</a>
</p>
<p>Email: <a href="mailto:cs1083@princeton.edu" target=&ldquo;blank&rdquo;>cs1083 at princeton dot edu</a>
</p>
<p>Phone: 434-218-9860
</p>
<p><a href="https://scholar.google.com/citations?user=twvDiW8AAAAJ&amp;hl=en" target=&ldquo;blank&rdquo;>Google Scholar Profile</a>
</p>
</td></tr></table>
<p>I am currently a Postdoctoral Fellow at the <a href="https://pli.princeton.edu/" target=&ldquo;blank&rdquo;>Princeton Language and Intelligence (PLI)</a> initiative at Princeton University, where I work closely with Professor <a href="https://sites.google.com/view/cjin/home" target=&ldquo;blank&rdquo;>Chi Jin</a>. Prior to joining PLI, I worked for a year as a Senior Machine Learning Research Engineer in the <a href="https://www.bloomberg.com/company/values/tech-at-bloomberg/artificial-intelligence-ai/" target=&ldquo;blank&rdquo;>AI group at Bloomberg</a>, New York City.
</p>
<p>I received my Ph.D. in Electrical Engineering from the University of Virginia in 2024, where I was advised by Professor <a href="https://cshen317.github.io/" target=&ldquo;blank&rdquo;>Cong Shen</a>. During my Ph.D. (2021–2024), I was honored to be supported by the <a href="https://www.bloomberg.com/company/stories/announcing-bloomberg-data-science-ph-d-fellowship-winners-2021-2022" target=&ldquo;blank&rdquo;>Bloomberg Data Science Ph.D. fellowship</a>.
</p>
<p>My research interests lie in machine learning, with a focus on intelligent decision-making. Specifically, I work on:
</p>
<ul>
<li><p>Foundational principles in areas such as reinforcement learning, multi-armed bandits, game theory, and multi-agent systems;
</p>
</li>
<li><p>Applications in emerging fields including wireless communication, recommender systems, and large language models
</p>
</li>
</ul>
<h2>News</h2>
<ul>
<li><p>01/2026: One paper accepted to <font color="Red">ICLR 2026</font>!
</p>
<ul>
<li><p>&ldquo;Efficient Multi-objective Prompt Optimization via Pure-exploration Bandits&rdquo;: This work studies multi-objective prompt optimization under a budget constraint using tools from pure-exploration bandits. It extends the framework introduced in <a href="https://arxiv.org/abs/2402.09723" target=&ldquo;blank&rdquo;>our previous work</a> (NeurIPS 2024), which established a connection between bandit algorithms and prompt optimization.
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>10/2025: Will attend the <a href="https://eastrailblazers.caltech.edu/symposiums/2025-trailblazers-symposium" target=&ldquo;blank&rdquo;>2025 EAS Trailblazers Symposium</a> at Caltech as one of the seven trailblazers!
</p>
</li>
</ul>
<ul>
<li><p>09/2025: One paper accepted to <font color="Red">NeurIPS 2025</font>!
</p>
<ul>
<li><p>&ldquo;<a href="https://arxiv.org/abs/2510.24700" target=&rdquo;blank&ldquo;>Greedy Sampling Is Provably Efficient For RLHF</a>&rdquo;: We show that, under both the Bradley–Terry model and a more general preference model, greedy sampling based on empirical estimates is provably efficient for RLHF with the KL-regularized objective.
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>09/2025: I have joined Princeton University as a Postdoctoral Fellow at the <a href="https://pli.princeton.edu/" target=&ldquo;blank&rdquo;>Princeton Language and Intelligence (PLI)</a> initiative! Feeling excited for this new adventure!
</p>
</li>
</ul>
<ul>
<li><p>05/2025: One paper accepted to <font color="Red">UAI 2025</font>!
</p>
<ul>
<li><p>&ldquo;<a href="https://arxiv.org/abs/2505.13768" target=&rdquo;blank&ldquo;>Augmenting Online RL with Offline Data is All You Need: A Unified Hybrid RL Algorithm Design and Analysis</a>&rdquo;: We propose a unified RL approach that combines offline data with online exploration, yielding improved sub-optimality and regret bounds while revealing distinct coverage requirements.
</p>
</li>
</ul>

</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2026-01-26 11:52:25 EST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
